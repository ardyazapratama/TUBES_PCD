import zipfile
import os
# 0. Extract ZIP File
zip_path = '/content/drive/MyDrive/TB.zip'  # Path to your ZIP file
extract_path = '/content/TB'         # Path where files will be extracted

# Check if the dataset is already extracted
if not os.path.exists(extract_path):
    os.makedirs(extract_path)

if zipfile.is_zipfile(zip_path):
    print(f"Extracting {zip_path}...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print("Extraction complete.")
else:
    print(f"{zip_path} is not a valid ZIP file.")\
# 1. Dataset Preparation
train_dir = os.path.join(extract_path, 'train')
val_dir = os.path.join(extract_path, 'val')
test_dir = os.path.join(extract_path, 'test')
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import random

# Image Data Generators for Data Augmentation
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_test_datagen = ImageDataGenerator(rescale=1.0/255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

val_generator = val_test_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

test_generator = val_test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

# 2. Load Pre-trained ResNet and Fine-Tuning
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
for layer in base_model.layers[:-20]:  # Freeze the first N layers
    layer.trainable = False

# Add Custom Layers on Top
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=predictions)

# Compile Model
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Define EarlyStopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',   # Metric to monitor (e.g., 'val_loss' or 'val_accuracy')
    patience=5,           # Number of epochs to wait without improvement
    restore_best_weights=True  # Restore model weights from the best epoch

)

# 3. Training the Model
history = model.fit(
    train_generator,
    epochs=20,
    validation_data=val_generator,
    callbacks=[early_stopping]
)

# 4. Evaluate the Model
test_steps = test_generator.samples // test_generator.batch_size
test_loss, test_acc = model.evaluate(test_generator, steps=test_steps)

# Predictions and Metrics
y_true = test_generator.classes
y_pred = (model.predict(test_generator) > 0.5).astype("int32").flatten()
# Calculate Metrics
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

# Sensitivity (Recall)
sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0

# Specificity
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0

# Precision
precision = tp / (tp + fp) if (tp + fp) > 0 else 0

# Negative Predictive Value (NPV)
npv = tn / (tn + fn) if (tn + fn) > 0 else 0

# Accuracy
accuracy = (tp + tn) / (tp + tn + fp + fn)

# F1 Score
f1_score = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0

# Print Metrics
print("\nAdditional Metrics:")
print(f"Sensitivity (Recall): {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Negative Predictive Value (NPV): {npv:.2f}")
print(f"Accuracy: {accuracy:.2f}")
print(f"F1 Score: {f1_score:.2f}")

print("Confusion Matrix:")
print(confusion_matrix(y_true, y_pred))
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=['Non-TB', 'TB']))

# Get a batch of test images and their true labels
test_images, test_labels = next(test_generator)
predictions = model.predict(test_images)
predicted_labels = (predictions > 0.5).astype("int32").flatten()

# 5. Visualize Training History
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('Loss')

# Plot a random selection of 10 test images with their true and predicted labels
plt.figure(figsize=(15, 10))
for i in range(10):
    idx = random.randint(0, len(test_images) - 1)
    image = test_images[idx]
    true_label = "TB" if test_labels[idx] == 1 else "Non-TB"
    predicted_label = "TB" if predicted_labels[idx] == 1 else "Non-TB"

    plt.subplot(2, 5, i + 1)
    plt.imshow(image)
    plt.title(f"True: {true_label}\nPred: {predicted_label}", fontsize=10)
    plt.axis('off')

plt.tight_layout()
plt.show()
